{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Identify Fraud from Enron Email\n",
    "by: Murat Gürcü\n",
    "\n",
    "Company: Airbus\n",
    "\n",
    "Country: Germany (Munich)\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. In this project, I will play detective, and put my new skills to use by building a person of interest identifier based on financial and email data made public as a result of the Enron scandal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Questions\n",
    "\n",
    "__1.) Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]__\n",
    "\n",
    "The main goal of this project is to classify who is a Person of Interest (POI) and who not. Therefore I will create a prediction model, which easily and fast can classify POI´s and non POIS´s. But first I will start with a short summary of our Enron dataset:\n",
    "\n",
    "Concerning our outcome at Lesson 6 our dataset has 146 entries (or employees) with 21 features each person. 18 of this person are POI´s and 128 are non-POI´s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Follwing an overview of our dataset:__\n",
    "\n",
    "Index: 146 entries\n",
    "\n",
    "Data columns (total 21 columns):\n",
    "\n",
    "Name | Non-Null | Type |\n",
    "--- | --- | --- |\n",
    "salary | 95 | non-null float64 \n",
    "to_messages | 86 | non-null float64 \n",
    "deferral_payments | 39 | non-null float64 \n",
    "total_payments | 125 | non-null float64 \n",
    "exercised_stock_options | 102 | non-null float64 \n",
    "bonus | 82 | non-null float64 \n",
    "restricted_stock | 110 | non-null float64 \n",
    "shared_receipt_with_poi | 86 | non-null float64 \n",
    "restricted_stock_deferred | 18 | non-null float64 \n",
    "total_stock_value | 126 | non-null float64 \n",
    "expenses | 95 | non-null float64 \n",
    "loan_advances | 4 | non-null float64 \n",
    "from_messages | 86 | non-null float64 \n",
    "other | 93 | non-null float64 \n",
    "from_this_person_to_poi | 86 | non-null float64 \n",
    "poi | 146 | non-null bool \n",
    "director_fees | 17 | non-null float64 \n",
    "deferred_income | 49 | non-null float64 \n",
    "long_term_incentive | 66 | non-null float64 \n",
    "email_address | 111 | non-null object \n",
    "from_poi_to_this_person | 86 | non-null float64 \n",
    "dtypes: bool(1), float64(19), object(1)\n",
    "memory usage: 24.1+ KB\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the identificiation of the outliers we will analyze the total stock values.\n",
    "\n",
    "Here is the graph with outliers:\n",
    "\n",
    "<img src=\"total_stock_values-salary_w outliers.PNG\">\n",
    "\n",
    "##### TOP TOTAL STOCK VALUE #\n",
    "Name|Value|\n",
    "---|----|\n",
    "TOTAL    |             434509511.0\n",
    "LAY KENNETH L |         49110078.0\n",
    "HIRKO JOSEPH   |        30766064.0\n",
    "SKILLING JEFFREY K |    26093672.0\n",
    "PAI LOU L          |    23817930.0\n",
    "\n",
    "Already here we can see, that _Total_ is an outlier. To identify more outliers we will add a new column with missing percentages.\n",
    "\n",
    "##### TOP MISSING VALUES RECORDS #\n",
    "Name|Value|\n",
    "---|----|\n",
    "LOCKHART EUGENE E  |              95.238095\n",
    "WROBEL BRUCE        |             85.714286\n",
    "THE TRAVEL AGENCY IN THE PARK |   85.714286\n",
    "GRAMM WENDY L                  |  85.714286\n",
    "WHALEY DAVID A                 |  85.714286\n",
    "\n",
    "Now we will also remove _The Travel Agency in the Park_ as it is not an individual and lot of values are missing. Additionally we will remove _Lockhart Eugene E_ as it has 95% of the coulmns empty.\n",
    "\n",
    "Now our new index is 143 entries.\n",
    "\n",
    "Allocation without outliers:\n",
    "\n",
    "<img src=\"total_stock_values-salary_wo outliers.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__2.) What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we will update our feature list by creating two new features, __messages_to_poi__ and __messages_from_poi__ and as well deleting features with too many null entires. \n",
    "\n",
    "The new feature messages_to_poi shows the ratio a person send emails to POI and messages_from_poi emails sent from POI.\n",
    "\n",
    "As we remember in Question 1 features like __loan_advances, director_fees, restricted_stock_deferred and deferral_payments__ have the highest missing entries and will be removed therefore.\n",
    "\n",
    "Our new Dataframe after update looks now:\n",
    "\n",
    "Index: 143 entries\n",
    "\n",
    "Data columns (total 20 columns):\n",
    "\n",
    "Name | Non-Null | Type |\n",
    "--- | --- | --- |\n",
    "salary | 94 |non-null float64\n",
    "to_messages | 86 |non-null float64\n",
    "total_payments | 123 |non-null float64\n",
    "exercised_stock_options | 101 |non-null float64\n",
    "bonus | 81 |non-null float64\n",
    "restricted_stock | 109| non-null float64\n",
    "shared_receipt_with_poi | 86| non-null float64\n",
    "total_stock_value | 125| non-null float64\n",
    "expenses | 94 | non-null float64\n",
    "from_messages | 86 | non-null float64\n",
    "other | 91 | non-null float64\n",
    "from_this_person_to_poi | 86 | non-null float64\n",
    "poi | 143 | non-null bool\n",
    "deferred_income | 48 | non-null float64\n",
    "long_term_incentive | 65 | non-null float64\n",
    "email_address | 111 | non-null object\n",
    "from_poi_to_this_person | 86 | non-null float64\n",
    "percent_missing | 143 | non-null float64\n",
    "messages_to_poi  | 86 | non-null float64\n",
    "messages_from_poi | 86 | non-null float64\n",
    "dtypes: bool(1), float64(18), object(1)\n",
    "memory usage: 22.5+ KB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will also check the raws for the missing values. We we will delete rows with more then 75% missing values.\n",
    "\n",
    "These persons are:\n",
    "\n",
    "- 'CHAN RONNIE',\n",
    "- 'WHALEY DAVID A',\n",
    "- 'CLINE KENNETH W', \n",
    "- 'WAKEHAM JOHN',\n",
    "- 'WROBEL BRUCE', \n",
    "- 'SAVAGE FRANK', \n",
    "- 'GRAMM WENDY L'\n",
    "\n",
    "Our Dataframe after cleaning looks now: \n",
    "\n",
    "Index: __136 entries__\n",
    "\n",
    "Data columns (total 20 columns):\n",
    "\n",
    "Name | Non-Null | Type |\n",
    "--- | --- | --- |\n",
    "salary             |        94 | non-null float64 \n",
    "to_messages         |       86 | non-null float64 \n",
    "total_payments       |      120 | non-null float64 \n",
    "exercised_stock_options |    99 | non-null float64 \n",
    "bonus                    |  81 | non-null float64 \n",
    "restricted_stock        |   107 | non-null float64 \n",
    "shared_receipt_with_poi  |  86 | non-null float64 \n",
    "total_stock_value |         122 | non-null float64 \n",
    "expenses           |        93 |non-null float64 \n",
    "from_messages       |       86|  non-null float64 \n",
    "other                |      91 | non-null float64 \n",
    "from_this_person_to_poi |    86 | non-null float64 \n",
    "poi           |             136 | non-null bool \n",
    "deferred_income |            46 |non-null float64 \n",
    "long_term_incentive |       65 | non-null float64 \n",
    "email_address        |      111 | non-null object \n",
    "from_poi_to_this_person |   86 | non-null float64 \n",
    "percent_missing          |  136 | non-null float64\n",
    "messages_to_poi        |    86 | non-null float64 \n",
    "messages_from_poi       |   86 | non-null float64 \n",
    "dtypes: bool(1), float64(18), object(1)\n",
    "\n",
    "For the missing values we will take the __mean__ of it´s column. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have chosen the scikit-learn __SelectKBest__ to select the best influential features. SelectKBest is selecting features according to the highest k scores (see also scikit-learn.org - sklearn.feature_selection). Before running SelectKBest we have scaled our dataset with the sklearn.preprocessing.scale which standardize our dataset along any axis (see also scikit-learn.org - sklearn.preprocessing.scale).\n",
    "\n",
    "#### BEST FEATURES \n",
    "\n",
    "Score | Feature Name | \n",
    "--- | --- | \n",
    "27.44 | exercised_stock_options \n",
    "19.97 | total_stock_value \n",
    "12.72 | messages_to_poi \n",
    "10.89 | bonus \n",
    "8.95 | salary \n",
    "7.19 | total_payments\n",
    "6.37 | restricted_stock \n",
    "5.68 | long_term_incentive \n",
    "5.46 | shared_receipt_with_poi \n",
    "4.91 | deferred_income \n",
    "2.89 | from_poi_to_this_person \n",
    "1.82 | other \n",
    "1.30 | from_this_person_to_poi \n",
    "1.11 | messages_from_poi \n",
    "0.56 | from_messages \n",
    "0.55 | expenses \n",
    "0.35 | to_messages \n",
    "\n",
    "As we can assess here only 2 of first 10 features are from fraction of Email, rest is from finance fraction. Also our engineered features have different scores. \"Messages to POI\" is on third place and therefore also significant for further investigations, while \"Messages from POI\" is not so significant and on 14th place. We didn´t fix the K value yet, because we want to be flexibel and in case also use our engineered feature \"Messages from POI\" (with a K of 15). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__3.) What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A perfect system combined with high Precision and high Recall will provide many correctly labeled results. I also apply this approach for my selection. But we need to highlight, that the size of our dataset is too small. So with our current train/test split our results will be dramatically impacted (i.e. 1 POI in our test possible). So we need to run our model with different split combinations, as the case in test_classifier(). In the following table the results are shown for 1 run and for 1000 runs (as in test_classifier()).\n",
    "\n",
    "I tried three different algorithms. Here is summary of my models and their accuracy, prediction and recall results:\n",
    "\n",
    "Model | Accuracy | Precision (1.000 runs) | Recall (1.000 runs) | Precision (1 run) | Recall (1 run) |\n",
    "--- | --- | --- | --- | --- | --- |\n",
    "Logistic Regression | 0.84207 | 0.40294 | 0.21900 | 0.60000 | 0.60000 | chosen model\n",
    "SVM | no result | no result | no result | 0.50000 | 0.20000 | due to the small sample size, the result by 1.000 runs are divided by zero and therefore not delivering any results\n",
    "Desicion Tree | 0.80750 | 0.33237 | 0.34450 | 0.666667 | 0.80000 |\n",
    "\n",
    "\n",
    "__Conclusion:__ depending on results of 1 run and 1.000 runs and on Accuracy rates, which is the weighted arithmetic mean of Precision and Recall, I go for Logistic Regression as it has less deviation between 1 run and 1.000 runs and the highest Accuracy. SVM is not showing any results on 1.000 runs, due to the small sample size. Decision tree has a good Precision and Recall at 1.000 runs, but a less Accuracy rate and more volatility between 1 run and 1.000 runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__4.) What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tuning parameters of an algorithm is a final step to get the best results. To optimizatize the parameters, which are impacting the model to get the best results, tuning suggests the search-nature of the problem. With a poor quality of tuning we will also get a low level of Performance and our model will be overfitted and biased. In Scikit-learn we can find different search methods. Easy two methods are Grid-Search and Random Search.\n",
    "\n",
    "I´ve applied the Grid-Search Method, which is systematically creating and evaluating a model for every combination of parameter specified in a grid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__5.) What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we check for a definition of validation we can find following in Wikipedia:\n",
    "\n",
    ".\n",
    "\n",
    "<font color='blue'> Cross-validation, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (testing dataset). The goal of cross validation is to define a dataset to \"test\" the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting, give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem), etc. </font>\n",
    ".\n",
    "\n",
    "<font color='blue'> One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds. </font>\n",
    "\n",
    ".\n",
    "\n",
    "<font color='blue'> One of the main reasons for using cross-validation instead of using the conventional validation (e.g. partitioning the data set into two sets of 70% for training and 30% for test) is that there is not enough data available to partition it into separate training and test sets without losing significant modelling or testing capability. In these cases, a fair way to properly estimate model prediction performance is to use cross-validation as a powerful general technique. </font>\n",
    "\n",
    ".\n",
    "\n",
    "<font color='blue'> In summary, cross-validation combines (averages) measures of fit (prediction error) to derive a more accurate estimate of model prediction performance. </font>\n",
    "\n",
    "To avoid the mistake of overfitting, I tried to keep my approach simple by tuning just a few parameters, and built function called _tune_and_eval_clf()_ in which I applied cross validation technique _sklearn.model_selection.StratifiedShuffleSplit()_ to split the data into training data and test data 10 times, calculate the accuracy, precision, and recall of each iteration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.) Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the evaluation of my model I´ve used 3 metrics: accuracy, precision, and recall. \n",
    "\n",
    "__Accuray__ is the weighted arithmetic mean of Precision and Recall. Official definition from Wikipedia:\n",
    "Accuracy has two definitions:\n",
    "1. More commonly, it is a description of systematic errors, a measure of statistical bias; as these cause a difference between a result and a \"true\" value, ISO calls this trueness.\n",
    "2. Alternatively, ISO defines accuracy as describing a combination of both types of observational error above (random and systematic), so high accuracy requires both high precision and high trueness.\n",
    "\n",
    "__Precision__: is the number of true positives over the number of true positives and additionally the number of false positives.\n",
    "\n",
    "__Recall__: ist the number of true positives over the number of trure positives and additionally the number of false negatives.\n",
    "\n",
    "Following the explanation of Wikipedia:\n",
    "<img src=\"Precisionrecall_svg.PNG\">\n",
    "\n",
    "As already listed in 3.) I have decided to take __Logistic Regression__ as it has less deviation between 1 run and 1.000 runs and the highest Accuracy. An accuracy ratio of 0.84207 means that the proportion of true results (both true positives and true negatives) is 0.84207 among the total number of cases examined. A precision of 0.40294 means that among the total 100 persons classified as POIs, 40 persons are actually POIs. A recall of 0.21900 means that among 100 true POIs existing in the dataset, ~22 POIs are correctly classified as POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References:\n",
    "- https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "- https://en.wikipedia.org/wiki/Accuracy_and_precision\n",
    "- https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "- http://scikit-learn.org\n",
    "- Udacity course Intro to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Attachments:\n",
    "- Main function poi_id2.py\n",
    "- Tester file tester2.py\n",
    "- Support functions enron2.py and feature_format.py\n",
    "- Pickle files (final_project_dataset, my_classifier, my_dataset, my_feature_list)\n",
    "- Dataset files (my_dataframe.xlsl)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
